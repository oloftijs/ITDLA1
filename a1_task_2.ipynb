{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "498542d3",
   "metadata": {},
   "source": [
    "# Task 2: Develop a “Tell-the-time” network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d263b",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "The problem of correctly telling the time can be formulated either as a multi-class classification\n",
    "problem (for example, with 12x60=720 classes representing each minute label) or a regression problem\n",
    "(for example, predicting the number of minutes after 12 o’clock). Therefore, your goal is to come up\n",
    "with different representations for the labels of your data adapt the output layer of your neural network\n",
    "and see how it impacts the training time and performance. No matter which architecture and loss\n",
    "function you will use when reporting results also provide “common sense” accuracy: the absolute value\n",
    "of the time difference between the predicted and the actual time (e.g., the “common sense” difference\n",
    "between “predicted” 11:55 and the “target” 0:05 is just 10 minutes and not 11 hours and 50 minutes!).\n",
    "Minimizing this “common sense” error measure is the main objective of this assignment! Notice that\n",
    "it is a common situation in Machine Learning: we often train models using one error measure (e.g.,\n",
    "cross-entropy loss) while the actual performance measure that we are interested in is different, e.g., the\n",
    "accuracy (the percentage of correctly classified cases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192faa0b",
   "metadata": {},
   "source": [
    "The dataset can be downloaded from here and it consists of 18000 grayscale images (18000x150x150\n",
    "or 18000x75x75) contained in ‘images.npy’. The labels for each sample are represented by two integers\n",
    "(18000x2, ‘labels.npy’ file), that correspond to the hour and minute displayed by the clock. You can see\n",
    "that each image is rendered from a different angle and rotation and they might contain light reflections from\n",
    "within the scene making this a non-trivial problem. For your experiments, we suggest splitting your data\n",
    "into 80/10/10% splits for training/validation and test sets respectively. Remember to shuffle your dataset\n",
    "as the sample files are ordered. We suggest using the smaller dataset for your initial tests and runs (75x75\n",
    "images) and then reporting your results on the larger (150x150) datase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ad866b",
   "metadata": {},
   "source": [
    "### (a) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd90345d",
   "metadata": {},
   "source": [
    "Treat this as a n-class classification problem. We suggest starting out with a\n",
    "smaller number of categories e.g. grouping all the samples that are between [3 : 00 − 3 : 30] into\n",
    "a single category (results in 24 categories in total), and trying to train a CNN model. Once you\n",
    "have found a working architecture, increase the number of categories by using smaller intervals\n",
    "for grouping samples to increase the ’common sense accuracy’. Can you train a network using\n",
    "all 720 different labels? What problems does such a label representation have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "421d4afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 16:17:41.183088: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-22 16:17:41.233931: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-22 16:17:42.322240: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from tensorflow import keras\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f46e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"A1_data_75\"\n",
    "images_path = os.path.join(data_folder, \"images.npy\")\n",
    "images = np.load(images_path)\n",
    "labels_path = os.path.join(data_folder, \"labels.npy\")\n",
    "labels = np.load(labels_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526553d",
   "metadata": {},
   "source": [
    "For your experiments, we suggest splitting your data\n",
    "into 80/10/10% splits for training/validation and test sets respectively. Remember to shuffle your dataset\n",
    "as the sample files are ordered. We suggest using the smaller dataset for your initial tests and runs (75x75\n",
    "images) and then reporting your results on the larger (150x150) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599be172",
   "metadata": {},
   "source": [
    "The problem of correctly telling the time can be formulated either as a multi-class classification\n",
    "problem (for example, with 12x60=720 classes representing each minute label) or a regression problem\n",
    "(for example, predicting the number of minutes after 12 o’clock). Therefore, your goal is to come up\n",
    "with different representations for the labels of your data adapt the output layer of your neural network\n",
    "and see how it impacts the training time and performance. No matter which architecture and loss\n",
    "function you will use when reporting results also provide “common sense” accuracy: the absolute value\n",
    "of the time difference between the predicted and the actual time (e.g., the “common sense” difference\n",
    "between “predicted” 11:55 and the “target” 0:05 is just 10 minutes and not 11 hours and 50 minutes!).\n",
    "Minimizing this “common sense” error measure is the main objective of this assignment! Notice that\n",
    "it is a common situation in Machine Learning: we often train models using one error measure (e.g.,\n",
    "cross-entropy loss) while the actual performance measure that we are interested in is different, e.g., the\n",
    "accuracy (the percentage of correctly classified cases). Here are some ideas that you should experiment\n",
    "with when building your models:\n",
    "2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df874d1",
   "metadata": {},
   "source": [
    "(a) Classification - treat this as a n-class classification problem. We suggest starting out with a\n",
    "smaller number of categories e.g. grouping all the samples that are between [3 : 00 −3 : 30] into\n",
    "a single category (results in 24 categories in total), and trying to train a CNN model. Once you\n",
    "have found a working architecture, increase the number of categories by using smaller intervals\n",
    "for grouping samples to increase the ’common sense accuracy’. Can you train a network using\n",
    "all 720 different labels? What problems does such a label representation have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65b7cc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0]\n",
      " [ 0  0]\n",
      " [ 0  0]\n",
      " ...\n",
      " [11 59]\n",
      " [11 59]\n",
      " [11 59]]\n",
      "[ 0  0  0 ... 23 23 23]\n"
     ]
    }
   ],
   "source": [
    "#  start with just using the hour labels as classification labels\n",
    "# extract the hour (first column) from the labels array\n",
    "print(labels)\n",
    "def get_cat_labels(labels):\n",
    "    new_labels = []\n",
    "    for label in labels:\n",
    "        label = label[0]* 2 + int(label[1] >= 30)\n",
    "        new_labels.append(label)\n",
    "    return np.array(new_labels)\n",
    "labels = get_cat_labels(labels)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de4abc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9e7c86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow detected 1 GPU(s):\n",
      "  - /physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"TensorFlow detected {len(gpus)} GPU(s):\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  - {gpu.name}\")\n",
    "else:\n",
    "    print(\"TensorFlow did NOT detect any GPUs. It will use the CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0460928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (14400, 75, 75)\n",
      "y_train shape: (14400,)\n",
      "X_valid shape: (1800, 75, 75)\n",
      "y_valid shape: (1800,)\n",
      "X_test shape: (1800, 75, 75)\n",
      "y_test shape: (1800,)\n"
     ]
    }
   ],
   "source": [
    "X_train_full, X_test,y_train_full, y_test = train_test_split(\n",
    "    images, labels, test_size=0.1, random_state=35\n",
    ")\n",
    "X_train, X_valid,y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=1/9, random_state=35\n",
    ") # 1/9 x 0.9 = 0.1. train test split shuffles by default\n",
    "\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_valid shape:\", X_valid.shape)\n",
    "print(\"y_valid shape:\", y_valid.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "printl = labels.reshape(-1, 1)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "235f20a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                 Version\n",
      "----------------------- -----------\n",
      "absl-py                 2.3.1\n",
      "asttokens               3.0.0\n",
      "astunparse              1.6.3\n",
      "Bottleneck              1.4.2\n",
      "brotlicffi              1.0.9.2\n",
      "certifi                 2025.10.5\n",
      "cffi                    2.0.0\n",
      "charset-normalizer      3.4.4\n",
      "comm                    0.2.3\n",
      "debugpy                 1.8.16\n",
      "decorator               5.2.1\n",
      "exceptiongroup          1.3.0\n",
      "executing               2.2.1\n",
      "filelock                3.17.0\n",
      "flatbuffers             25.9.23\n",
      "fsspec                  2025.7.0\n",
      "gast                    0.6.0\n",
      "gmpy2                   2.2.1\n",
      "google-pasta            0.2.0\n",
      "grpcio                  1.76.0\n",
      "h5py                    3.15.1\n",
      "idna                    3.11\n",
      "importlib_metadata      8.7.0\n",
      "ipykernel               7.0.1\n",
      "ipython                 9.6.0\n",
      "ipython_pygments_lexers 1.1.1\n",
      "jedi                    0.19.2\n",
      "Jinja2                  3.1.6\n",
      "joblib                  1.5.2\n",
      "jupyter_client          8.6.3\n",
      "jupyter_core            5.9.1\n",
      "keras                   3.11.3\n",
      "libclang                18.1.1\n",
      "Markdown                3.9\n",
      "markdown-it-py          4.0.0\n",
      "MarkupSafe              3.0.3\n",
      "matplotlib-inline       0.1.7\n",
      "mdurl                   0.1.2\n",
      "mkl-service             2.4.0\n",
      "ml_dtypes               0.5.3\n",
      "mpi4py                  4.0.3\n",
      "mpmath                  1.3.0\n",
      "namex                   0.1.0\n",
      "nest_asyncio            1.6.0\n",
      "networkx                3.5\n",
      "numexpr                 2.11.0\n",
      "numpy                   2.3.4\n",
      "opentelemetry-api       1.37.0\n",
      "opt_einsum              3.4.0\n",
      "optree                  0.17.0\n",
      "packaging               25.0\n",
      "pandas                  2.3.3\n",
      "parso                   0.8.5\n",
      "pexpect                 4.9.0\n",
      "pickleshare             0.7.5\n",
      "pillow                  12.0.0\n",
      "pip                     25.2\n",
      "platformdirs            4.5.0\n",
      "prompt_toolkit          3.0.52\n",
      "protobuf                6.33.0\n",
      "psutil                  7.0.0\n",
      "ptyprocess              0.7.0\n",
      "pure_eval               0.2.3\n",
      "pycparser               2.23\n",
      "Pygments                2.19.2\n",
      "PySocks                 1.7.1\n",
      "python-dateutil         2.9.0.post0\n",
      "pytz                    2025.2\n",
      "pyzmq                   27.1.0\n",
      "requests                2.32.5\n",
      "rich                    14.2.0\n",
      "scikit-learn            1.7.2\n",
      "scipy                   1.16.2\n",
      "setuptools              80.9.0\n",
      "six                     1.17.0\n",
      "stack_data              0.6.3\n",
      "sympy                   1.14.0\n",
      "tensorboard             2.20.0\n",
      "tensorboard-data-server 0.7.2\n",
      "tensorflow              2.20.0\n",
      "termcolor               3.1.0\n",
      "threadpoolctl           3.5.0\n",
      "torch                   2.6.0\n",
      "tornado                 6.5.1\n",
      "traitlets               5.14.3\n",
      "typing_extensions       4.15.0\n",
      "tzdata                  2025.2\n",
      "urllib3                 2.5.0\n",
      "wcwidth                 0.2.14\n",
      "Werkzeug                3.1.3\n",
      "wheel                   0.45.1\n",
      "wrapt                   2.0.0\n",
      "zipp                    3.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2865ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_sense_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    y_pred_class = tf.argmax(y_pred, axis=1)\n",
    "    y_true_float = tf.cast(tf.squeeze(y_true), dtype=tf.float32)\n",
    "    y_pred_float = tf.cast(y_pred_class, dtype=tf.float32)\n",
    "    diff = tf.abs(y_true_float - y_pred_float)\n",
    "    cyclical_diff = tf.minimum(diff, 12.0 - diff)\n",
    "    # Return the mean of these cyclical differences for the batch\n",
    "    return tf.reduce_mean(cyclical_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92fb4aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1761142665.995530    7134 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6119 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2025-10-22 16:17:46.333888: W external/local_xla/xla/service/gpu/llvm_gpu_backend/default/nvptx_libdevice_path.cc:41] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  ipykernel_launcher.runfiles/cuda_nvcc\n",
      "  ipykernel_launcher.runfiles/cuda_nvdisasm\n",
      "  ipykernel_launcher.runfiles/nvidia_nvshmem\n",
      "  ipykern/cuda_nvcc\n",
      "  ipykern/cuda_nvdisasm\n",
      "  ipykern/nvidia_nvshmem\n",
      "  \n",
      "  /usr/local/cuda\n",
      "  /opt/cuda\n",
      "  /home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
      "  /home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
      "  /home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/tensorflow/python/platform/../../cuda\n",
      "  /home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/tensorflow/python/platform/../../../../../..\n",
      "  /home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/tensorflow/python/platform/../../../../../../..\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 16:17:50.029328: I external/local_xla/xla/service/service.cc:163] XLA service 0x7eeba00032c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-10-22 16:17:50.029376: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2025-10-22 16:17:50.132618: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-10-22 16:17:50.672520: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91400\n",
      "2025-10-22 16:17:50.899013: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-22 16:17:50.899131: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-22 16:17:50.899149: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-10-22 16:17:51.787170: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1928', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2025-10-22 16:17:52.103719: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2698', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-10-22 16:17:52.274962: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2682', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-10-22 16:17:56.198093: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2025-10-22 16:17:56.215305: W tensorflow/core/framework/op_kernel.cc:1855] OP_REQUIRES failed at xla_ops.cc:590 : INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "2025-10-22 16:17:56.215357: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INTERNAL: libdevice not found at ./libdevice.10.bc\n",
      "\t [[{{node StatefulPartitionedCall}}]]\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 758, in start\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 701, in shell_main\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 469, in dispatch_shell\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 379, in execute_request\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 899, in execute_request\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 471, in do_execute\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 632, in run_cell\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n\n  File \"/tmp/ipykernel_7134/2277636739.py\", line 41, in <module>\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\nlibdevice not found at ./libdevice.10.bc\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_6143]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m      3\u001b[39m model = keras.models.Sequential([\n\u001b[32m      4\u001b[39m     keras.Input(shape=(\u001b[32m75\u001b[39m, \u001b[32m75\u001b[39m, \u001b[32m1\u001b[39m)),\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Block 1\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     keras.layers.Dense(\u001b[32m24\u001b[39m, activation=\u001b[33m\"\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m ])\n\u001b[32m     36\u001b[39m model.compile(loss=\u001b[33m'\u001b[39m\u001b[33msparse_categorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     37\u001b[39m optimizer=keras.optimizers.Adam(learning_rate=\u001b[32m0.001\u001b[39m),\n\u001b[32m     38\u001b[39m metrics=[common_sense_loss,\u001b[33m\"\u001b[39m\u001b[33mAccuracy\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     39\u001b[39m         \u001b[38;5;66;03m#   tf.keras.metrics.Precision(), tf.keras.metrics.Recall()\u001b[39;00m\n\u001b[32m     40\u001b[39m           ])\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# callbacks=[early_stopping]\u001b[39;49;00m\n\u001b[32m     46\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m#evaluate the model on the test set\u001b[39;00m\n\u001b[32m     48\u001b[39m test_loss,test_csl, test_acc = model.evaluate(X_test, y_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/idl-fix/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/idl-fix/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[32m     54\u001b[39m                                       inputs, attrs, num_outputs)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mInternalError\u001b[39m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 758, in start\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 701, in shell_main\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 469, in dispatch_shell\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 379, in execute_request\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 899, in execute_request\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 471, in do_execute\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 632, in run_cell\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n\n  File \"/tmp/ipykernel_7134/2277636739.py\", line 41, in <module>\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/ollie/anaconda3/envs/idl-fix/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\nlibdevice not found at ./libdevice.10.bc\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_6143]"
     ]
    }
   ],
   "source": [
    "max_pool = keras.layers.MaxPool2D(pool_size=2)\n",
    "# avg_pool = keras.layers.AveragePooling2D(pool_size=2)\n",
    "model = keras.models.Sequential([\n",
    "    keras.Input(shape=(75, 75, 1)),\n",
    "    # Block 1\n",
    "    keras.layers.Conv2D(32, (3,3), activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D((2,2)), # Output shape: (37, 37, 32)\n",
    "\n",
    "    # Block 2\n",
    "    keras.layers.Conv2D(64, (3,3), activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(64, (3,3), activation=\"relu\", padding=\"same\"), # <-- Note: 64 filters again\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(2), # Output shape: (18, 18, 64)\n",
    "\n",
    "    # Block 3\n",
    "    keras.layers.Conv2D(128, (3,3), activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(128, (3,3), activation=\"relu\", padding=\"same\"), # <-- Note: 128 filters again\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(2), # Output shape: (9, 9, 128)\n",
    "\n",
    "    # Block 4\n",
    "    keras.layers.Conv2D(256, (3,3), activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(2), # Output shape: (4, 4, 256)\n",
    "\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation=\"leaky_relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(64, activation=\"leaky_relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(24, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "metrics=[common_sense_loss,\"Accuracy\"\n",
    "        #   tf.keras.metrics.Precision(), tf.keras.metrics.Recall()\n",
    "          ])\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    # callbacks=[early_stopping]\n",
    ")\n",
    "#evaluate the model on the test set\n",
    "test_loss,test_csl, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "#base:0.8420000076293945\n",
    "#leaky: 0.8525000214576721\n",
    "#leaky + L2regularization: 0.8472999930381775\n",
    "#leaky + batch normalization: 0.8978000283241272\n",
    "\n",
    "(print(tf.__version__))\n",
    "# metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138eec7d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6306a9e",
   "metadata": {},
   "source": [
    "### (b) Regression\n",
    "\n",
    "try to build a network that predicts the time using a single output node in the\n",
    "following format: [”03 : 00” → y = 3.0]; [”05 : 30” → y = 5.5], where categorical labels of hours\n",
    "and minutes get transformed to a single continuous value. What kind of loss function would you\n",
    "need to use for such a task? What kind of problems does such a representation have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cc6f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67102df4",
   "metadata": {},
   "source": [
    "### (c) Multi-head models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49100e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36461ab8",
   "metadata": {},
   "source": [
    "### (d) Label transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba594a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7173647d",
   "metadata": {},
   "source": [
    "## 2\n",
    "Use the knowledge gained by working with other datasets in the previous parts of this assignment to\n",
    "optimize your final models and decrease the error of telling the time as much as possible (common\n",
    "sense error of below 10 minutes is achievable using relatively simple CNN architectures). You should\n",
    "also compare the different ways of representing your labels and different neural network output layer\n",
    "combinations. You should use an 80:20% ratio for the train/test sets respectively. Document your\n",
    "experiments and findings in the report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
